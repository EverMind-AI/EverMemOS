"""
æ¢å¤ Stage2 Embedding ç”Ÿæˆï¼ˆä»Žæ–­ç‚¹ç»§ç»­ï¼‰

å¦‚æžœ stage2_index_building.py ä¸­é€”å¡ä½ï¼Œä½¿ç”¨æ­¤è„šæœ¬ä»Žæ–­ç‚¹ç»§ç»­ã€‚
"""

import json
import os
import sys
import pickle
import asyncio
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, "..", ".."))
SRC_DIR = os.path.abspath(os.path.join(PROJECT_ROOT, "src"))
sys.path.insert(0, PROJECT_ROOT)
sys.path.insert(0, SRC_DIR)

from evaluation.locomo_evaluation.config import ExperimentConfig
from src.agentic_layer import vectorize_service


async def resume_build_emb_index(config: ExperimentConfig, data_dir: Path, emb_save_dir: Path, start_from: int = 0):
    """
    ä»ŽæŒ‡å®šçš„ conversation å¼€å§‹æž„å»º embedding ç´¢å¼•
    
    Args:
        config: å®žéªŒé…ç½®
        data_dir: memcells æ•°æ®ç›®å½•
        emb_save_dir: embedding ä¿å­˜ç›®å½•
        start_from: ä»Žå“ªä¸ª conversation å¼€å§‹ï¼ˆ0-basedï¼‰
    """
    # ðŸ”¥ ä¼˜åŒ–åŽçš„å‚æ•°
    BATCH_SIZE = 100  # æ›´å°çš„æ‰¹æ¬¡ï¼Œæ›´å¤šå¹¶å‘æœºä¼š
    MAX_CONCURRENT_BATCHES = 10
    
    print(f"\n{'='*60}")
    print(f"Resuming Embedding Generation from Conv {start_from}")
    print(f"{'='*60}\n")

    for i in range(start_from, config.num_conv):
        file_path = data_dir / f"memcell_list_conv_{i}.json"
        if not file_path.exists():
            print(f"Warning: File not found, skipping: {file_path}")
            continue

        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        output_path = emb_save_dir / f"embedding_index_conv_{i}.pkl"
        if output_path.exists():
            print(f"âœ… Conv {i} already completed, skipping...")
            continue

        print(f"\n{'='*60}")
        print(f"Processing {file_path.name} for embedding...")
        print(f"{'='*60}")

        with open(file_path, "r", encoding="utf-8") as f:
            original_docs = json.load(f)

        texts_to_embed = []
        doc_field_map = []
        for doc_idx, doc in enumerate(original_docs):
            # ä¼˜å…ˆä½¿ç”¨event_logï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
            if doc.get("event_log") and doc["event_log"].get("atomic_fact"):
                atomic_facts = doc["event_log"]["atomic_fact"]
                if isinstance(atomic_facts, list) and atomic_facts:
                    for fact_idx, fact in enumerate(atomic_facts):
                        if fact and isinstance(fact, str) and fact.strip():
                            texts_to_embed.append(fact)
                            doc_field_map.append((doc_idx, f"atomic_fact_{fact_idx}"))
                    continue

            # å›žé€€åˆ°åŽŸæœ‰å­—æ®µï¼ˆä¿æŒå‘åŽå…¼å®¹ï¼‰
            for field in ["subject", "summary", "episode"]:
                if text := doc.get(field):
                    texts_to_embed.append(text)
                    doc_field_map.append((doc_idx, field))

        if not texts_to_embed:
            print(f"Warning: No documents found in {file_path.name}. Skipping embedding creation.")
            continue

        total_texts = len(texts_to_embed)
        total_batches = (total_texts + BATCH_SIZE - 1) // BATCH_SIZE
        print(f"Total texts to embed: {total_texts}")
        print(f"Batch size: {BATCH_SIZE}")
        print(f"Total batches: {total_batches}")
        print(f"Max concurrent batches: {MAX_CONCURRENT_BATCHES}")
        print(f"\nStarting parallel embedding generation...")
        
        # ðŸ”¥ å¹¶å‘æ‰¹æ¬¡å¤„ç†
        start_time = time.time()
        
        async def process_batch(batch_idx: int, batch_texts: list) -> tuple[int, list]:
            """å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ˆå¼‚æ­¥ï¼‰"""
            try:
                batch_embeddings = await vectorize_service.get_text_embeddings(batch_texts)
                print(f"  âœ“ Batch {batch_idx + 1}/{total_batches} complete ({len(batch_texts)} texts)")
                return (batch_idx, batch_embeddings)
            except Exception as e:
                print(f"  âŒ Batch {batch_idx + 1}/{total_batches} failed: {e}")
                return (batch_idx, [])
        
        # åˆ›å»ºæ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
        tasks = []
        for j in range(0, total_texts, BATCH_SIZE):
            batch_idx = j // BATCH_SIZE
            batch_texts = texts_to_embed[j : j + BATCH_SIZE]
            task = process_batch(batch_idx, batch_texts)
            tasks.append(task)
        
        print(f"Submitting {len(tasks)} batches for concurrent processing...")
        
        # åˆ†æ‰¹æäº¤ä»»åŠ¡ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        batch_results = []
        completed = 0
        chunk_size = MAX_CONCURRENT_BATCHES * 2
        
        for chunk_start in range(0, len(tasks), chunk_size):
            chunk_tasks = tasks[chunk_start : chunk_start + chunk_size]
            chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=False)
            batch_results.extend(chunk_results)
            
            completed += len(chunk_tasks)
            progress = (completed / len(tasks)) * 100
            print(f"  Progress: {completed}/{len(tasks)} batches ({progress:.1f}%)")
        
        # æŒ‰æ‰¹æ¬¡é¡ºåºé‡ç»„ç»“æžœ
        all_embeddings = []
        for batch_idx, batch_embeddings in sorted(batch_results, key=lambda x: x[0]):
            all_embeddings.extend(batch_embeddings)
        
        elapsed_time = time.time() - start_time
        speed = total_texts / elapsed_time if elapsed_time > 0 else 0
        print(f"\nâœ… Embedding generation complete!")
        print(f"   - Total texts: {total_texts}")
        print(f"   - Total embeddings: {len(all_embeddings)}")
        print(f"   - Time elapsed: {elapsed_time:.2f}s")
        print(f"   - Speed: {speed:.1f} texts/sec")
        print(f"   - Average batch time: {elapsed_time/total_batches:.2f}s")
        
        # éªŒè¯ç»“æžœå®Œæ•´æ€§
        if len(all_embeddings) != total_texts:
            print(f"   âš ï¸  Warning: Expected {total_texts} embeddings, got {len(all_embeddings)}")
        else:
            print(f"   âœ“ All embeddings generated successfully")

        # é‡ç»„ embeddings
        doc_embeddings = [{"doc": doc, "embeddings": {}} for doc in original_docs]
        
        for (doc_idx, field), emb in zip(doc_field_map, all_embeddings):
            if field.startswith("atomic_fact_"):
                if "atomic_facts" not in doc_embeddings[doc_idx]["embeddings"]:
                    doc_embeddings[doc_idx]["embeddings"]["atomic_facts"] = []
                doc_embeddings[doc_idx]["embeddings"]["atomic_facts"].append(emb)
            else:
                doc_embeddings[doc_idx]["embeddings"][field] = emb

        # ä¿å­˜ç»“æžœ
        emb_save_dir.mkdir(parents=True, exist_ok=True)
        print(f"Saving embeddings to: {output_path}")
        with open(output_path, "wb") as f:
            pickle.dump(doc_embeddings, f)
        
        print(f"âœ… Conv {i} completed and saved!")


async def main():
    """ä¸»å‡½æ•°"""
    config = ExperimentConfig()
    data_dir = Path(__file__).parent / "results" / config.experiment_name / "memcells"
    emb_save_dir = Path(__file__).parent / "results" / config.experiment_name / "vectors"
    
    # ðŸ”¥ æ£€æŸ¥å·²å®Œæˆçš„ conversationï¼Œè‡ªåŠ¨ä»Žæ–­ç‚¹ç»§ç»­
    start_from = 0
    for i in range(config.num_conv):
        output_path = emb_save_dir / f"embedding_index_conv_{i}.pkl"
        if output_path.exists():
            start_from = i + 1
        else:
            break
    
    if start_from >= config.num_conv:
        print(f"âœ… All conversations already completed!")
        return
    
    print(f"ðŸ”„ Resuming from Conv {start_from}")
    await resume_build_emb_index(config, data_dir, emb_save_dir, start_from=start_from)
    
    print(f"\n{'='*60}")
    print(f"âœ… All embedding generation complete!")
    print(f"{'='*60}")


if __name__ == "__main__":
    asyncio.run(main())

